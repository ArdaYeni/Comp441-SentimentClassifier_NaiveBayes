{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "## I unzipped the tar.gz file as above. File name should be data in order to perform correctly\n",
        "##Also for better perfomance, it should be run on google collab\n",
        "##Arda Burak Yeni 76688\n",
        "\n",
        "\n",
        "\n",
        "#import tarfile\n",
        "\n",
        "# open file\n",
        "#file = tarfile.open('aclImdb_v1.tar.gz')\n",
        "\n",
        "# extracting file\n",
        "##file.extractall('data')\n",
        "\n",
        "#file.close()\n",
        "\n",
        "# I created a set for words which doesnt have sentimental efffect for text\n",
        "stop_words = set([\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\",\n",
        "    \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\",\n",
        "    \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\",\n",
        "    \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
        "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\",\n",
        "    \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\",\n",
        "    \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
        "    \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
        "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\",\n",
        "    \"between\", \"into\", \"through\", \"during\", \"before\", \"after\",\n",
        "    \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\",\n",
        "    \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\",\n",
        "    \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\",\n",
        "    \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\",\n",
        "    \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\",\n",
        "    \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n",
        "    \"don\", \"should\", \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\",\n",
        "    \"ain\", \"aren\", \"couldn\", \"didn\", \"doesn\", \"hadn\", \"hasn\",\n",
        "    \"haven\", \"isn\", \"ma\", \"mightn\", \"mustn\", \"needn\", \"shan\",\n",
        "    \"shouldn\", \"wasn\", \"weren\", \"won\", \"wouldn\"\n",
        "])\n",
        "\n",
        "# This function loads the positive and negative datasets\n",
        "def load_data(data_dir):\n",
        "    texts = []\n",
        "    sentiments = []\n",
        "\n",
        "    for sentiment in ['pos', 'neg']:\n",
        "        sentiment_dir = os.path.join(data_dir, sentiment)\n",
        "        for filename in os.listdir(sentiment_dir):\n",
        "            with open(os.path.join(sentiment_dir, filename), 'r', encoding='utf-8') as file:\n",
        "                texts.append(file.read())\n",
        "                sentiments.append('positive' if sentiment == 'pos' else 'negative')\n",
        "\n",
        "    return texts, sentiments\n",
        "\n",
        "# I initiliazed the train and test datasets\n",
        "train_texts, train_sentiments = load_data('data/aclImdb/train')\n",
        "test_texts, test_sentiments = load_data('data/aclImdb/test')\n",
        "\n",
        "# Tokenization and preprocessing\n",
        "def preprocessing(text):\n",
        "    # tokinizind and making lowerspace\n",
        "    tokens = text.lower().split()\n",
        "    new_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "           new_tokens.append(token)\n",
        "\n",
        "    return new_tokens\n",
        "\n",
        "# This functions builds a vocabulary with frequency threshold and for the words below threshold it puts \"unk\"\n",
        "def build_vocabulary(texts, frequency_threshold=5):\n",
        "    word_freq = {}\n",
        "\n",
        "    for text in texts:\n",
        "        tokens = preprocessing(text)\n",
        "        for token in tokens:\n",
        "            if token in word_freq:\n",
        "                word_freq[token] += 1\n",
        "            else:\n",
        "                word_freq[token] = 1\n",
        "\n",
        "    vocab = {'padding': 0, 'unk': 1}\n",
        "    word_index = 2\n",
        "\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= frequency_threshold:\n",
        "            vocab[word] = word_index\n",
        "            word_index += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "\n",
        "#This functions converts texts to the ingetegers which explained in descriptions\n",
        "def texts_to_sequences(texts, vocab):\n",
        "    sequences = []\n",
        "    for text in texts:\n",
        "        tokens = preprocessing(text)\n",
        "        sequence = []\n",
        "        for token in tokens:\n",
        "            # Tokens are being converted to their index in vocab\n",
        "            if token in vocab:\n",
        "                index = vocab[token]\n",
        "            else:\n",
        "                index = vocab['unk'] # unk if token is not in vocab\n",
        "\n",
        "            sequence.append(index)\n",
        "\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# For better accuracy I cropped the sequences like mentioned in the descriptions\n",
        "def pad_sequences(sequences, max_length):\n",
        "    padded_sequences = []\n",
        "\n",
        "    for seq in sequences:\n",
        "        if len(seq) > max_length:\n",
        "            padded_seq = seq[:max_length]  # Crop longer sequences\n",
        "        else:\n",
        "            padded_seq = seq + [0] * (max_length - len(seq))  # Pad shorter with \"padding\"\n",
        "\n",
        "        padded_sequences.append(padded_seq)\n",
        "\n",
        "    return np.array(padded_sequences)\n",
        "\n",
        "# 1. Build the vocabulary\n",
        "vocab = build_vocabulary(train_texts, frequency_threshold=5)\n",
        "\n",
        "# 2. converting sets to the integer sequences\n",
        "train_sequences = texts_to_sequences(train_texts, vocab)\n",
        "test_sequences = texts_to_sequences(test_texts, vocab)\n",
        "\n",
        "# 3. fixed length for cropping\n",
        "#for max_length 50 -> 79 accuracy\n",
        "#for 200 -> 83.9%\n",
        "#300->83.92\n",
        "#400 -> 83.7\n",
        "max_length = 200\n",
        "train_padded = pad_sequences(train_sequences, max_length)\n",
        "test_padded = pad_sequences(test_sequences, max_length)\n",
        "\n",
        "\n",
        "def calculate_word_counts(sequences, sentiments, vocab):\n",
        "\n",
        "    word_count = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "\n",
        "    for i in range(len(sequences)):\n",
        "        seq = sequences[i]  # Get the current sequence\n",
        "        sentiment = sentiments[i]  # Get the corresponding sentiment\n",
        "\n",
        "        for word_idx in seq:\n",
        "\n",
        "            if word_idx != 0:  # If the word index is not padding\n",
        "                # Increment the count for this word index and sentiment class\n",
        "                word_count[word_idx][sentiment] += 1\n",
        "\n",
        "    return word_count\n",
        "word_counts = calculate_word_counts(train_padded, train_sentiments, vocab)\n",
        "\n",
        "# 5. Calculate log-likelihood and log-priors for each class\n",
        "def calculating_loglikelihood(word_counts, total_word_count, vocab_size, laplacian_smoothing=1):\n",
        "    log_likelihood = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    for word_idx, class_count in word_counts.items():\n",
        "        for sentiment in ['positive', 'negative']:\n",
        "            # Laplician smoothing for 0 probabilities\n",
        "            log_likelihood[word_idx][sentiment] = math.log((class_count[sentiment] + laplacian_smoothing) /\n",
        "                                                           (total_word_count[sentiment] + laplacian_smoothing * vocab_size))\n",
        "\n",
        "    return log_likelihood\n",
        "\n",
        "#I initiliazed dictionary for sentiment count for both negative and positive\n",
        "total_word_count = {\n",
        "    'positive': 0,  # Initialize total for positive words\n",
        "    'negative': 0   # Initialize total for negative words\n",
        "}\n",
        "\n",
        "# Iterate through words\n",
        "for word_index, counts in word_counts.items():\n",
        "\n",
        "    if 'positive' in counts:\n",
        "        total_word_count['positive'] += counts['positive']\n",
        "    if 'negative' in counts:\n",
        "\n",
        "        total_word_count['negative'] += counts['negative']\n",
        "\n",
        "\n",
        "\n",
        "# Calculate log likelihoods\n",
        "log_likelihood = calculating_loglikelihood(word_counts, total_word_count, len(vocab))\n",
        "\n",
        "# Calculate log priors\n",
        "def calculate_log_prior(sentiments):\n",
        "    total_texts = len(sentiments)\n",
        "    log_prior_positive = math.log(sentiments.count('positive') / total_texts)\n",
        "    log_prior_negative = math.log(sentiments.count('negative') / total_texts)\n",
        "\n",
        "    return log_prior_positive, log_prior_negative\n",
        "\n",
        "log_prior_positive, log_prior_negative = calculate_log_prior(train_sentiments)\n",
        "\n",
        "# 6. Predicting sentiment\n",
        "def predict(sequence, log_likelihood, log_prior_positive, log_prior_negative):\n",
        "    log_score_positive = log_prior_positive\n",
        "    log_score_negative = log_prior_negative\n",
        "\n",
        "    for word_idx in sequence:\n",
        "        if word_idx != 0:  # Ignore padding\n",
        "            log_score_positive += log_likelihood[word_idx]['positive']\n",
        "            log_score_negative += log_likelihood[word_idx]['negative']\n",
        "\n",
        "    return 'positive' if log_score_positive > log_score_negative else 'negative'\n",
        "\n",
        "# 7. The accuracy calculation\n",
        "correct_predictions = 0\n",
        "\n",
        "total_samples = len(test_sentiments)\n",
        "for i in range(total_samples):\n",
        "    sequence = test_padded[i]\n",
        "    true_sentiment = test_sentiments[i]\n",
        "\n",
        "\n",
        "    predicted_sentiment = predict(sequence, log_likelihood, log_prior_positive, log_prior_negative)\n",
        "    if predicted_sentiment == true_sentiment:\n",
        "        correct_predictions += 1\n",
        "accuracy = (correct_predictions / total_samples) * 100\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVXZuMDWtZj6",
        "outputId": "b36f67bf-cbec-4ea9-cf33-c6c229cfdcc4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 83.90%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
